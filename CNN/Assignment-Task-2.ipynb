{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural style transfer\n",
    "\n",
    "In this task, you will create a fancy photo for your own interest by applying the neural style transfer theory you learned in the lecture and also the uploaded Jupyter Notebook. Of course, it would not a be meaningfull task if the task asks you to run the already written code using your own images.\n",
    "\n",
    "## Task\n",
    "\n",
    "You will need perform the following tasks in one combined approach.\n",
    "\n",
    "- In the uploaded Jupyter Notebook, the model VGG-19 was used to perform neural style transfer. You need to use a VGG-16 to perform neural style transfer. This is a good place to start with [VGG-16](https://pytorch.org/vision/main/models/generated/torchvision.models.vgg16.html)\n",
    "- In the uploaded Jupyter Notebook, I used all activations in five convolution layers for computing the content loss function $\\mathcal{L}_\\text{content}(C, G)$ and the style loss function $\\mathcal{L}_\\text{style}(S, G)$. However, the theory says that as for the content loss function, we only need one single convolution layer to compute the distance between the content image and the generated image and this layer should be quite deep. It is actually possible to use the deepest convolution layer right after the max pooling layer (see the uploaded Jupyter Notebook) to compute the content loss function $\\mathcal{L}_\\text{content}(C,G)$. So you need to modify the uploaded code so that you use several convolution layers (as many as you want) to compute the style loss function $\\mathcal{L}_\\text{style}(S, G)$ but use only one single convolution layer to compute the content loss function $\\mathcal{L}_\\text{content}(C, G)$.\n",
    "\n",
    "In summary, you need to use the VGG-16 convolution neural network [VGG-16](https://pytorch.org/vision/main/models/generated/torchvision.models.vgg16.html) and to use only one convolution layer to compute the content loss function $\\mathcal{L}_\\text{content}(C, G)$. \n",
    "\n",
    "**Hint**\n",
    "\n",
    "- It is advisable to use the convolution layers right after the pooling layers. As for the content loss function, it is good enough to use the convolution layer right after the last pooling layer. To understand what I mean, you need to download the VGG-16 just like how the uploaded Jupyter Notebook illustrated and then examine the architecture of the VGG-16 convolution neural network. You will understand what I mean.\n",
    "\n",
    "- Although this task sounds complicated, the code modification is very little and you can create your own art work (Â¬â€¿Â¬).\n",
    "\n",
    "- You may want to play around with the content weight and the style weight. But generally, the style weight should be quite big so that the style can \"dip\" into the content image. *I normally choose style-weight vs content-weight ratio to be around a few thousand to a a few hundred thousounds or even a million.* Normally, you see the result quite clearly after $20$ or $30$ epochs. Just save the figures after a certain number of epochs. Then, you can continue or stop the process to modify the weights. However, you should not worry too much about it because it proves to me that the final result will be quite artistic if things are done correctly.\n",
    "\n",
    "- If you see the code runs too slow, you can reduce the size of the image so that that the loss function has fewer number of variables to optimize. For example, instead of the image of size $(400, 600)$, you can create the image of size $(300, 450)$ or even smaller. I don't mind if you submit a rather small figure. It is only important that you understand the theory and write the code well. You can choose the height-vs-width ratio as you want as long as it looks nice.\n",
    "\n",
    "### VGG-16 architecture vs VGG-19 architecture\n",
    "\n",
    "You can learn about the architecture of a deep neural network in PyTorch by just using a simple statement `print(model)`. After downloading the VGG-16 model or VGG-19 model, you can print them out to examine their architecture. Here are their visual illustration.\n",
    "\n",
    "**VGG-16 architecture**\n",
    "\n",
    "<img src=\"./figures/vgg-16-architecture.png\" alt=\"uog+vangogh\" width=\"600\"/> \n",
    "\n",
    "**VGG-19 architecture**\n",
    "\n",
    "<img src=\"./figures/vgg-19-architecture.png\" alt=\"uog+romantic\" width=\"700\"/>\n",
    "\n",
    "## Evidence of work\n",
    "\n",
    "You must submit your content image, your style image and your generated image together with your code as the evidence of work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My own artwork\n",
    "\n",
    "The following are the artworks I created by using VGG-16 model and only one convolution layer for computing the content loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='magenta'><u>Example 1</u></font>\n",
    "\n",
    "**Content image**\n",
    "\n",
    "<img src=\"./figures/uog.jpg\" alt=\"uog\" width=\"450\"/>\n",
    "\n",
    "**Style image**\n",
    "\n",
    "<img src=\"./figures/vangogh.jpg\" alt=\"vangogh\" width=\"450\"/>  <img src=\"./figures/romantic.jpg\" alt=\"romantic\" width=\"450\"/> \n",
    "\n",
    "\n",
    "**Generated image**\n",
    "\n",
    "<img src=\"./figures/uog+vangogh-VGG16-10.jpg\" alt=\"uog+vangogh\" width=\"450\"/>  <img src=\"./figures/uog+romantic-VGG16-10.jpg\" alt=\"uog+romantic\" width=\"450\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='magenta'><u>Example 3</u></font>\n",
    "\n",
    "I just picked up this photo online and used the same style images as above. But you can do this with your boyfriend or your girlfriend ðŸ˜‹\n",
    "\n",
    "**Content image**\n",
    "\n",
    "<img src=\"./figures/couple.jpg\" alt=\"uog\" width=\"450\"/>\n",
    "\n",
    "**Generated image**\n",
    "\n",
    "<img src=\"./figures/couple+vangogh-VGG16.jpg\" alt=\"couple+vangogh\" width=\"450\"/> <img src=\"./figures/couple+romantic-400x600-VGG16.jpg\" alt=\"couple+vangogh\" width=\"450\"/>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
